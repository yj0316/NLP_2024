{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yj0316/NLP_2024/blob/main/14_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üê£ üê• **Tokenization**"
      ],
      "metadata": {
        "id": "8YnDKrw00anR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9jvterL0Tj-",
        "outputId": "bc18e9e0-8448-478a-89c1-56c1494457e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "#!pip install nltk is a command to install the NLTK library. The installation is a one-time process.\n",
        "!pip install nltk\n",
        "\n",
        "#'import nltk' is a statement to make the NLTK library available for use in your Python code. The import statement is used in each script or notebook where you want to utilize NLTK functionality.\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ms624atyale/NLP_2024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auTClxv05FXm",
        "outputId": "feac0e36-cc5d-429f-b09e-c40450d55a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP_2024'...\n",
            "remote: Enumerating objects: 582, done.\u001b[K\n",
            "remote: Counting objects: 100% (417/417), done.\u001b[K\n",
            "remote: Compressing objects: 100% (234/234), done.\u001b[K\n",
            "remote: Total 582 (delta 281), reused 270 (delta 181), pack-reused 165 (from 1)\u001b[K\n",
            "Receiving objects: 100% (582/582), 15.09 MiB | 9.88 MiB/s, done.\n",
            "Resolving deltas: 100% (330/330), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path to your CSV file\n",
        "#üëâ Modify\n",
        "file_path = \"/content/NLP_2024/Sample_ClassActivity/4_Aesop4Children_CSV_BodyONY_Sample4ClassActivity.csv\" #üëâ Modify\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Print the list of columns in the CSV file\n",
        "    print(\"List of columns:\", df.columns.tolist())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file at '{file_path}' was not found.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Error: The file is empty.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error: The file could not be parsed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcfdtW5PBGJ_",
        "outputId": "9804d90a-af67-454f-d84c-3e86c7eb6e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of columns: ['Text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üê£üê£ <font color = 'green'>Word2Vec (Frequently used word pairs)**"
      ],
      "metadata": {
        "id": "CjBSdrH_JW24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color = 'green'> Use the following script for semantically related word pairs by the Word2Vec class.**"
      ],
      "metadata": {
        "id": "9uoXYVN2WwpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') #üëâ ADDed\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 1: Load and Verify the CSV File\n",
        "def load_and_verify_csv(file_path, text_column):\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Print original column names to verify\n",
        "        print(\"Original Column Names:\", df.columns.tolist())\n",
        "\n",
        "        # Standardize column names: strip spaces and convert to lowercase\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "        print(\"Standardized Column Names:\", df.columns.tolist())\n",
        "\n",
        "        # Check if the text column exists\n",
        "        if text_column.lower() not in df.columns:\n",
        "            raise KeyError(f\"Column '{text_column}' not found in the CSV file.\")\n",
        "\n",
        "        # Extract text data\n",
        "        text_list = df[text_column.lower()].dropna().tolist()\n",
        "        print(f\"Successfully loaded {len(text_list)} text entries from '{text_column}' column.\")\n",
        "        return text_list\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at '{file_path}' was not found.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The file is empty.\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Error: The file could not be parsed.\")\n",
        "    except KeyError as e:\n",
        "        print(e)\n",
        "\n",
        "# Step 2: Preprocess Text with Stopword Removal and Content Word Filtering\n",
        "def preprocess_text(text_list):\n",
        "    all_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))  # Adjust language as needed\n",
        "    for text in text_list:\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize words\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        # Remove stopwords and non-alphabetic tokens\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "        all_tokens.append(filtered_tokens)\n",
        "    return all_tokens\n",
        "\n",
        "# Step 3: Train Word2Vec Model\n",
        "def train_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4):\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
        "    print(\"Word2Vec model training completed.\")\n",
        "    return model\n",
        "\n",
        "# Step 4: Find Semantically Related Word Pairs Excluding Function Words\n",
        "def find_semantically_related_pairs(model, top_n=100):\n",
        "    words = list(model.wv.index_to_key)\n",
        "    word_pairs = []\n",
        "    stop_words = set(stopwords.words('english'))  # Reuse the stopwords set for filtering\n",
        "\n",
        "    # Using Gensim's most_similar method to find similar words\n",
        "    for word in words:\n",
        "        similar_words = model.wv.most_similar(word, topn=5)  # Adjust topn as needed\n",
        "        for similar_word, similarity in similar_words:\n",
        "            # Ensure both words are not in stopwords and are alphabetic\n",
        "            if similar_word in stop_words:\n",
        "                continue  # Skip if similar_word is a stopword\n",
        "            # To avoid duplicate pairs (word1-word2 and word2-word1), sort them\n",
        "            sorted_pair = tuple(sorted([word, similar_word]))\n",
        "            if sorted_pair not in word_pairs:\n",
        "                word_pairs.append((sorted_pair[0], sorted_pair[1], similarity))\n",
        "\n",
        "    # Sort the pairs based on similarity\n",
        "    word_pairs = sorted(word_pairs, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Return the top N pairs\n",
        "    return word_pairs[:top_n]\n",
        "\n",
        "# Step 5: Generate Network Graph\n",
        "def generate_network_graph(word_pairs, similarity_threshold=0.8, save_path=None):\n",
        "    \"\"\"\n",
        "    Generates a network graph from semantically related word pairs.\n",
        "\n",
        "    :param word_pairs: List of tuples (Word1, Word2, Similarity)\n",
        "    :param similarity_threshold: Minimum similarity score to include an edge\n",
        "    :param save_path: If provided, saves the graph to the specified path\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add edges with similarity above the threshold\n",
        "    for word1, word2, similarity in word_pairs:\n",
        "        if similarity >= similarity_threshold:\n",
        "            G.add_edge(word1, word2, weight=similarity)\n",
        "\n",
        "    if len(G.nodes) == 0:\n",
        "        print(\"No edges meet the similarity threshold. Adjust the threshold and try again.\")\n",
        "        return\n",
        "\n",
        "    # Set positions using spring layout\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges = G.edges(data=True)\n",
        "    weights = [edge_data['weight'] for _, _, edge_data in edges]\n",
        "\n",
        "    # Normalize weights for edge thickness\n",
        "    max_weight = max(weights) if weights else 1\n",
        "    normalized_weights = [weight / max_weight * 5 for weight in weights]  # Scale edge widths\n",
        "\n",
        "    # Draw nodes\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='lightblue')\n",
        "\n",
        "    # Draw edges with widths based on similarity\n",
        "    nx.draw_networkx_edges(G, pos, width=normalized_weights, alpha=0.7, edge_color='gray')\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')\n",
        "\n",
        "    plt.title(\"Semantically Related Word Pairs Network Graph\", fontsize=25)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, format='PNG')\n",
        "        print(f\"Network graph saved to '{save_path}'.\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Step 6: Main Function to Execute the Workflow\n",
        "def main():\n",
        "    # Define file path and text column\n",
        "    #üëâ Modify\n",
        "    file_path = \"/content/NLP_2024/Sample_ClassActivity/4_Aesop4Children_CSV_BodyONY_Sample4ClassActivity.csv\" #üëâ Modify\n",
        "    #üëâ Modify\n",
        "    text_column = \"Text\"  # Corrected to match the actual column name  #üëâ Modify\n",
        "\n",
        "    # Load and verify the CSV\n",
        "    text_list = load_and_verify_csv(file_path, text_column)\n",
        "    if not text_list:\n",
        "        print(\"No text data to process.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess the text with stopword removal and content word filtering\n",
        "    sentences = preprocess_text(text_list)\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    model = train_word2vec_model(sentences)\n",
        "\n",
        "    # Find semantically related word pairs excluding function words\n",
        "    top_n = 100  # Number of top related word pairs to display\n",
        "    related_word_pairs = find_semantically_related_pairs(model, top_n)\n",
        "\n",
        "    # Create a DataFrame from the related word pairs\n",
        "    df_pairs = pd.DataFrame(related_word_pairs, columns=['Word1', 'Word2', 'Similarity'])\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    output_file = \"/content/semantically_related_word_pairs.csv\"\n",
        "    df_pairs.to_csv(output_file, index=False)\n",
        "    print(f\"\\nTop {top_n} Semantically Related Word Pairs saved to '{output_file}'.\")\n",
        "\n",
        "    # Generate Network Graph\n",
        "    print(\"\\nGenerating network graph...\")\n",
        "    generate_network_graph(related_word_pairs, similarity_threshold=0.8, save_path=\"/content/semantically_related_word_pairs_graph.png\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l62gG8hW6PS",
        "outputId": "333e53ae-c6b2-44ed-afe3-e661318f2b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Column Names: ['Text']\n",
            "Standardized Column Names: ['text']\n",
            "Successfully loaded 4 text entries from 'Text' column.\n",
            "Word2Vec model training completed.\n",
            "\n",
            "Top 100 Semantically Related Word Pairs saved to '/content/semantically_related_word_pairs.csv'.\n",
            "\n",
            "Generating network graph...\n",
            "No edges meet the similarity threshold. Adjust the threshold and try again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üê£üê£ <font color = 'green'>Word2Vec & plotting (Frequently used word pairs)**"
      ],
      "metadata": {
        "id": "PjVf1tfhJrCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk gensim scikit-learn networkx matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbzrXCrBJyEx",
        "outputId": "0bbd5e4d-a052-4b93-a6c5-41b7a533f58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') #üëâ Added\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') #üëâ ADDed\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to map NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Step 1: Load and Verify the CSV File\n",
        "def load_and_verify_csv(file_path, text_column):\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Print original column names to verify\n",
        "        print(\"Original Column Names:\", df.columns.tolist())\n",
        "\n",
        "        # Standardize column names: strip spaces and convert to lowercase\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "        print(\"Standardized Column Names:\", df.columns.tolist())\n",
        "\n",
        "        # Check if the text column exists\n",
        "        if text_column.lower() not in df.columns:\n",
        "            raise KeyError(f\"Column '{text_column}' not found in the CSV file.\")\n",
        "\n",
        "        # Extract text data\n",
        "        text_list = df[text_column.lower()].dropna().tolist()\n",
        "        print(f\"Successfully loaded {len(text_list)} text entries from '{text_column}' column.\")\n",
        "        return text_list\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at '{file_path}' was not found.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The file is empty.\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Error: The file could not be parsed.\")\n",
        "    except KeyError as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "# Step 2: Preprocess Text with Stopword Removal and POS Filtering\n",
        "def preprocess_text_with_pos(text_list):\n",
        "    all_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for text in text_list:\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize words\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        # POS tagging\n",
        "        pos_tags = pos_tag(tokens)\n",
        "        # Filter tokens: remove stopwords, non-alphabetic, and keep only content POS\n",
        "        filtered_tokens = [\n",
        "            word for word, pos in pos_tags\n",
        "            if word not in stop_words and word.isalpha() and get_wordnet_pos(pos) is not None\n",
        "        ]\n",
        "        all_tokens.append(filtered_tokens)\n",
        "    return all_tokens\n",
        "\n",
        "# Step 3: Train Word2Vec Model\n",
        "def train_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4): #100 dimensionality, 11 words tested each (5 words to the left, target word, 5 words to the right)\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
        "    print(\"Word2Vec model training completed.\")\n",
        "    return model\n",
        "\n",
        "# Step 4: Find Semantically Related Word Pairs Excluding Function Words\n",
        "def find_semantically_related_pairs(model, top_n=50):\n",
        "    words = list(model.wv.index_to_key)\n",
        "    word_pairs = []\n",
        "    stop_words = set(stopwords.words('english'))  # Reuse the stopwords set for filtering\n",
        "\n",
        "    # Using Gensim's most_similar method to find similar words\n",
        "    for word in words:\n",
        "        similar_words = model.wv.most_similar(word, topn=5)  # Adjust topn as needed\n",
        "        for similar_word, similarity in similar_words:\n",
        "            # Ensure similar_word is not a stopword and is alphabetic\n",
        "            if similar_word in stop_words:\n",
        "                continue  # Skip if similar_word is a stopword\n",
        "            # To avoid duplicate pairs (word1-word2 and word2-word1), sort them\n",
        "            sorted_pair = tuple(sorted([word, similar_word]))\n",
        "            if sorted_pair not in word_pairs:\n",
        "                word_pairs.append((sorted_pair[0], sorted_pair[1], similarity))\n",
        "\n",
        "    # Sort the pairs based on similarity\n",
        "    word_pairs = sorted(word_pairs, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Return the top N pairs\n",
        "    return word_pairs[:top_n]\n",
        "\n",
        "# Step 5: Generate Network Graph\n",
        "def generate_network_graph(word_pairs, similarity_threshold=0.6, save_path=None):\n",
        "    \"\"\"\n",
        "    Generates a network graph from semantically related word pairs.\n",
        "\n",
        "    :param word_pairs: List of tuples (Word1, Word2, Similarity)\n",
        "    :param similarity_threshold: Minimum similarity score to include an edge\n",
        "    :param save_path: If provided, saves the graph to the specified path\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add edges with similarity above the threshold\n",
        "    for word1, word2, similarity in word_pairs:\n",
        "        if similarity >= similarity_threshold:\n",
        "            G.add_edge(word1, word2, weight=similarity)\n",
        "\n",
        "    if len(G.nodes) == 0:\n",
        "        print(\"No edges meet the similarity threshold. Adjust the threshold and try again.\")\n",
        "        return\n",
        "\n",
        "    # Set positions using spring layout\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges = G.edges(data=True)\n",
        "    weights = [edge_data['weight'] for _, _, edge_data in edges]\n",
        "\n",
        "    # Normalize weights for edge thickness\n",
        "    max_weight = max(weights) if weights else 1\n",
        "    normalized_weights = [weight / max_weight * 5 for weight in weights]  # Scale edge widths\n",
        "\n",
        "    # Draw nodes\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='lightblue')\n",
        "\n",
        "    # Draw edges with widths based on similarity\n",
        "    nx.draw_networkx_edges(G, pos, width=normalized_weights, alpha=0.7, edge_color='gray')\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')\n",
        "\n",
        "    plt.title(\"Semantically Related Word Pairs Network Graph\", fontsize=25)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, format='PNG')\n",
        "        print(f\"Network graph saved to '{save_path}'.\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Step 6: Main Function to Execute the Workflow\n",
        "def main():\n",
        "    # Define file path and text column\n",
        "    #üëâ Modify\n",
        "    file_path = \"/content/NLP_2024/Sample_ClassActivity/4_Aesop4Children_CSV_BodyONY_Sample4ClassActivity.csv\"  #üëâ Modify\n",
        "    #üëâ Modify\n",
        "    text_column = \"Text\"  # Correctly set to match the actual column name  #üëâ Modify\n",
        "\n",
        "    # Load and verify the CSV\n",
        "    text_list = load_and_verify_csv(file_path, text_column)\n",
        "    if not text_list:\n",
        "        print(\"No text data to process.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess the text with stopword removal and content word filtering\n",
        "    sentences = preprocess_text_with_pos(text_list)\n",
        "\n",
        "    # Debug: Print sample preprocessed sentences\n",
        "    print(\"\\nSample Preprocessed Sentences:\")\n",
        "    for i, sentence in enumerate(sentences[:3], 1):\n",
        "        print(f\"{i}: {sentence}\")\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    model = train_word2vec_model(sentences)\n",
        "\n",
        "    # Find semantically related word pairs excluding function words\n",
        "    top_n = 50  # Reduced to 50 for a smaller dataset\n",
        "    related_word_pairs = find_semantically_related_pairs(model, top_n)\n",
        "\n",
        "    # Debug: Print the found word pairs and their similarity scores\n",
        "    print(\"\\nSemantically Related Word Pairs:\")\n",
        "    for pair in related_word_pairs:\n",
        "        print(f\"{pair[0]} - {pair[1]}: Similarity = {pair[2]:.4f}\")\n",
        "\n",
        "    # Create a DataFrame from the related word pairs\n",
        "    df_pairs = pd.DataFrame(related_word_pairs, columns=['Word1', 'Word2', 'Similarity'])\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    output_file = \"/content/semantically_related_word_pairs.csv\"\n",
        "    df_pairs.to_csv(output_file, index=False)\n",
        "    print(f\"\\nTop {top_n} Semantically Related Word Pairs saved to '{output_file}'.\")\n",
        "\n",
        "    # Generate Network Graph with a lower similarity threshold\n",
        "    print(\"\\nGenerating network graph...\")\n",
        "    # If higher numbers do not generate graph, try to lower this value. For the text used here, lower than 0.4 will generate a graph. Here we use 0.3.\n",
        "    similarity_threshold = 0.3  #üëâ Modify: Lowered threshold to include more edges\n",
        "    generate_network_graph(related_word_pairs, similarity_threshold=similarity_threshold, save_path=\"/content/semantically_related_word_pairs_graph.png\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "57p2I6NqFpEp",
        "outputId": "26a26bb3-040e-4425-f6fc-2c658cca7026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file at '/content/NLP_2024/Sample_ClassActivity/4_Aesop4Children_CSV_BodyONY_Sample4ClassActivity.csv' was not found.\n",
            "No text data to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Step 1: Load and Verify the CSV File\n",
        "def load_and_verify_csv(file_path, text_column):\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Print original column names to verify\n",
        "        print(\"Original Column Names:\", df.columns.tolist())\n",
        "\n",
        "        # Standardize column names: strip spaces and convert to lowercase\n",
        "        df.columns = df.columns.str.strip().str.lower()\n",
        "        print(\"Standardized Column Names:\", df.columns.tolist())\n",
        "\n",
        "        # Check if the text column exists\n",
        "        if text_column.lower() not in df.columns:\n",
        "            raise KeyError(f\"Column '{text_column}' not found in the CSV file.\")\n",
        "\n",
        "        # Extract text data\n",
        "        text_list = df[text_column.lower()].dropna().tolist()\n",
        "        print(f\"Successfully loaded {len(text_list)} text entries from '{text_column}' column.\")\n",
        "        return text_list\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file at '{file_path}' was not found.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: The file is empty.\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"Error: The file could not be parsed.\")\n",
        "    except KeyError as e:\n",
        "        print(e)\n",
        "\n",
        "# Step 2: Preprocess Text with Stopword Removal\n",
        "def preprocess_text(text_list):\n",
        "    all_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))  # Adjust language as needed\n",
        "    for text in text_list:\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize words\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        # Remove stopwords\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "        all_tokens.append(filtered_tokens)\n",
        "    return all_tokens\n",
        "\n",
        "# Step 3: Train Word2Vec Model\n",
        "def train_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4):\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
        "    print(\"Word2Vec model training completed.\")\n",
        "    return model\n",
        "\n",
        "# Step 4: Find Semantically Related Word Pairs\n",
        "def find_semantically_related_pairs(model, top_n=100):\n",
        "    words = list(model.wv.index_to_key)\n",
        "    word_pairs = []\n",
        "\n",
        "    # Using Gensim's most_similar method to find similar words\n",
        "    for word in words:\n",
        "        similar_words = model.wv.most_similar(word, topn=5)  # Adjust topn as needed\n",
        "        for similar_word, similarity in similar_words:\n",
        "            # To avoid duplicate pairs (word1-word2 and word2-word1), sort them\n",
        "            sorted_pair = tuple(sorted([word, similar_word]))\n",
        "            if sorted_pair not in word_pairs:\n",
        "                word_pairs.append((sorted_pair[0], sorted_pair[1], similarity))\n",
        "\n",
        "    # Sort the pairs based on similarity\n",
        "    word_pairs = sorted(word_pairs, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Return the top N pairs\n",
        "    return word_pairs[:top_n]\n",
        "\n",
        "# Step 5: Generate Network Graph\n",
        "def generate_network_graph(word_pairs, similarity_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Generates a network graph from semantically related word pairs.\n",
        "\n",
        "    :param word_pairs: List of tuples (Word1, Word2, Similarity)\n",
        "    :param similarity_threshold: Minimum similarity score to include an edge\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add edges with similarity above the threshold\n",
        "    for word1, word2, similarity in word_pairs:\n",
        "        if similarity >= similarity_threshold:\n",
        "            G.add_edge(word1, word2, weight=similarity)\n",
        "\n",
        "    # Set positions using spring layout\n",
        "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges = G.edges(data=True)\n",
        "    weights = [edge_data['weight'] for _, _, edge_data in edges]\n",
        "\n",
        "    # Draw nodes\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue')\n",
        "\n",
        "    # Draw edges with widths based on similarity\n",
        "    nx.draw_networkx_edges(G, pos, width=[weight * 2 for weight in weights], alpha=0.6)\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')\n",
        "\n",
        "    plt.title(\"Semantically Related Word Pairs Network Graph\", fontsize=20)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Step 6: Main Function to Execute the Workflow\n",
        "def main():\n",
        "    # Define file path and text column\n",
        "    #üëâ Modify\n",
        "    file_path = \"/content/NLP_2024/Sample_ClassActivity/4_Aesop4Children_CSV_BodyONY_Sample4ClassActivity.csv\"\n",
        "    #üëâ Modify\n",
        "    text_column = \"Text\"  # Ensure this matches the actual column name #üëâ Modify\n",
        "\n",
        "    # Load and verify the CSV\n",
        "    text_list = load_and_verify_csv(file_path, text_column)\n",
        "    if not text_list:\n",
        "        print(\"No text data to process.\")\n",
        "        return\n",
        "\n",
        "    # Preprocess the text with stopword removal\n",
        "    sentences = preprocess_text(text_list)\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    model = train_word2vec_model(sentences)\n",
        "\n",
        "    # Find semantically related word pairs\n",
        "    top_n = 100  # Number of top related word pairs to display\n",
        "    related_word_pairs = find_semantically_related_pairs(model, top_n)\n",
        "\n",
        "    # Create a DataFrame from the related word pairs\n",
        "    df_pairs = pd.DataFrame(related_word_pairs, columns=['Word1', 'Word2', 'Similarity'])\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    output_file = \"/content/semantically_related_word_pairs.csv\"\n",
        "    df_pairs.to_csv(output_file, index=False)\n",
        "    print(f\"\\nTop {top_n} Semantically Related Word Pairs saved to '{output_file}'.\")\n",
        "\n",
        "    # Generate Network Graph\n",
        "    print(\"\\nGenerating network graph...\")\n",
        "    #üëâ Modify the value of threshold as you refer to values above...\n",
        "    generate_network_graph(related_word_pairs, similarity_threshold=0.3)  # Adjust threshold as needed\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV9ylo6XJqrA",
        "outputId": "a0207adf-d992-4855-806b-a432fdd33ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file at '/content/NLP_2024/Sample_ClassActivity/4_Aesop4Children_CSV_BodyONY_Sample4ClassActivity.csv' was not found.\n",
            "No text data to process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}